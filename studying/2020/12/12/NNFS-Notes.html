<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Notes From NNFS | Joshs Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Notes From NNFS" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Notes from NNFS" />
<meta property="og:description" content="Notes from NNFS" />
<link rel="canonical" href="https://jprier.github.io/josh-blog/studying/2020/12/12/NNFS-Notes.html" />
<meta property="og:url" content="https://jprier.github.io/josh-blog/studying/2020/12/12/NNFS-Notes.html" />
<meta property="og:site_name" content="Joshs Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-12T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://jprier.github.io/josh-blog/studying/2020/12/12/NNFS-Notes.html","@type":"BlogPosting","headline":"Notes From NNFS","dateModified":"2020-12-12T00:00:00-06:00","datePublished":"2020-12-12T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://jprier.github.io/josh-blog/studying/2020/12/12/NNFS-Notes.html"},"description":"Notes from NNFS","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/josh-blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jprier.github.io/josh-blog/feed.xml" title="Joshs Blog" /><link rel="shortcut icon" type="image/x-icon" href="/josh-blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/josh-blog/">Joshs Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/josh-blog/about/">About Me</a><a class="page-link" href="/josh-blog/search/">Search</a><a class="page-link" href="/josh-blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Notes From NNFS</h1><p class="page-description">Notes from NNFS</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-12-12T00:00:00-06:00" itemprop="datePublished">
        Dec 12, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/josh-blog/categories/#Studying">Studying</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#the-basics-of-a-layer">The Basics of a Layer</a></li>
<li class="toc-entry toc-h2"><a href="#batches">Batches</a></li>
<li class="toc-entry toc-h2"><a href="#activation-functions">Activation Functions</a>
<ul>
<li class="toc-entry toc-h3"><a href="#the-basics">The Basics</a></li>
<li class="toc-entry toc-h3"><a href="#step-function">Step Function</a></li>
<li class="toc-entry toc-h3"><a href="#linear-activation-function">Linear Activation Function</a></li>
<li class="toc-entry toc-h3"><a href="#sigmoid-function">Sigmoid Function</a></li>
<li class="toc-entry toc-h3"><a href="#relu">ReLU</a></li>
</ul>
</li>
</ul><p>Wanting to relearn the ML basics from my university degree I started going through the course <a href="https://nnfs.io/">Neural Networks From Scratch</a>.</p>

<p>This post contains my notes and learnings from the course.</p>

<h2 id="the-basics-of-a-layer">
<a class="anchor" href="#the-basics-of-a-layer" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Basics of a Layer</h2>

<p>A layer of a neural network contains 1 or more <em>neurons</em>, these neurons will take a vector (or matrix with batches) as input and have a vector of weights to multiply by each of the inputs and a bias to add to the input*weights.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nb">input</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>
<span class="n">bias</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># output of a single neuron
</span><span class="n">output</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">*</span><span class="n">weights</span> <span class="o">+</span> <span class="n">bias</span>
</code></pre></div></div>

<p>This is easily scaled up to multiple neurons with matrices</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]</span>
<span class="n">weights</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
           <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.91</span><span class="p">,</span> <span class="mf">0.26</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">],</span>
           <span class="p">[</span><span class="o">-</span><span class="mf">0.26</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.27</span><span class="p">,</span> <span class="mf">0.17</span><span class="p">,</span> <span class="mf">0.87</span><span class="p">]]</span>
<span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>

<span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span> <span class="o">+</span> <span class="n">biases</span>
</code></pre></div></div>

<blockquote>
  <p>Tip: numpy <code class="language-plaintext highlighter-rouge">shape</code> has an out to in order s.t. in <code class="language-plaintext highlighter-rouge">shape(x,y)</code> x is the size of the outmost dimension and y is the size of the most inner dimension</p>
</blockquote>

<p>Each layer is essentially the dot product the weights and the previous layer’s output (input data for the first layer)</p>

<h2 id="batches">
<a class="anchor" href="#batches" aria-hidden="true"><span class="octicon octicon-link"></span></a>Batches</h2>

<p>In the previous examples we take a vector as an input <code class="language-plaintext highlighter-rouge">input = [1,2]</code>. This vector is a single object in our dataset, this can be an image of a dog, the values a house sold, (x,y) values of a point, or anything else. Each value is a feature of the object, a feature describes some aspect of the object. In the case of points on a 2d graph the features for the objects would be the x and y coordinates of each point so our input in this case would be [x, y].</p>

<p>We do not want to pass a single object as input for each pass of the neural network, this would be slow and would take forever for large datasets. Instead of passing single objects we can pass multiple objects at once in what is called a <strong>batch</strong>.</p>

<p>A batch will contain 2 or more vectors, typically we have batch sizes of 16-64 with 32 being common.
Too small of a batch size will cause the neural network to underfit, this is due to the model not getting enough information in each pass causing it to make too large of a change to the parameters on each pass. 
Too large of a batch size will cause the neural network to overfit, this is due to it fitting to too much of the sample data at once which will cause the test accuracy to be much lower.</p>

<h2 id="activation-functions">
<a class="anchor" href="#activation-functions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Activation Functions</h2>

<h3 id="the-basics">
<a class="anchor" href="#the-basics" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Basics</h3>

<p>The activation function is a tool to adjust how and when a neuron of the network should activate. A network will typically use a different activation function between the output and hidden layers.</p>

<p>We use the activation function with the calculation of the weights, inputs, and bias per neuron</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">output</span> <span class="o">=</span> <span class="n">Activaiton</span><span class="p">(</span><span class="n">weights</span><span class="o">*</span><span class="nb">input</span> <span class="o">+</span> <span class="n">bias</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="step-function">
<a class="anchor" href="#step-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step Function</h3>
<p>A simple activation function is the step function</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">weights</span><span class="o">*</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">bias</span>
<span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span>
  <span class="n">y</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">else</span> 
  <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></div>
<p>The step function is a binary on or off for a neuron, 1 or 0 depending on the value of the calculation</p>

<h3 id="linear-activation-function">
<a class="anchor" href="#linear-activation-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear Activation Function</h3>

<h3 id="sigmoid-function">
<a class="anchor" href="#sigmoid-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Sigmoid Function</h3>

<h3 id="relu">
<a class="anchor" href="#relu" aria-hidden="true"><span class="octicon octicon-link"></span></a>ReLU</h3>

  </div><a class="u-url" href="/josh-blog/studying/2020/12/12/NNFS-Notes.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/josh-blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/josh-blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/josh-blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My Blog for Notes, Projects, and Rambles</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/josh-blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/josh-blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
